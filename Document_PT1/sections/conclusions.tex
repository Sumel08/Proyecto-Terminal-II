
\section{Conclusiones}

Durante el desarrollo de Proyecto Terminal I se lograron definir las técnicas y herramientas a usar para el desarrollo del sistema propuesto que surge de la falta de inclusión de personas que tienen problemas de audición a las actividades diarias que se desempeñan en la ciudad, para esto se propuso una aplicación móvil que permite llevar a cabo la comunicación entre personas que utilizan el lenguaje de señas con personas que no lo usan a través de un sistema de reconocimiento de voz que traduce las palabras habladas a imágenes del lenguaje de señas mexicano que corresponden a estas palabras.

En cuanto al reconocimiento de voz se determinaron las técnicas que se llevaran a cabo para cumplir con esta tarea, para el pre-procesamiento se definieron las características para la obtención de la señal de voz las cuales son una frecuencia d muestreo de 8 kHz y un PCM de 8 bits debido a que se requieren tiempos de procesamiento no mayores a 5 segundos para mantener la conversación fluida, se estableció el uso de un filtro preénfasis para resaltar las características frecuenciales de la señal de voz y se determinó el uso de la energía de la señal y tasa de cruces por cero (características de la señal en el dominio del tiempo) para llevar a cabo la detección de bordes de la señal y así eliminar los silencios. Se determinó el uso de la técnica de extracción de características MFCC debido a los buenos resultados obtenidos en los trabajos presentados en el estado del arte y finalmente se empleará la cuantización vectorial a los coeficientes obtenidos de la extracción de características, así se tendrá una longitud fija en el vector de características que servirá de entrada a la red neuronal.

De acuerdo a los trabajos analizados se determinó el uso de la arquitectura \textit{feedforward Pattern Recognition} y el algoritmo de entrenamiento \textit{Scaled Conjugate Gradient backpropagation} a emplear en la red neuronal que realizará el reconocimiento de patrones de los vectores de características y de esta forma determinar las palabras que corresponden a la señal de voz, como funciones de activación se emplearan \textit{sigmoid} para la capa oculta y \textit{softmax} para la capa de salida.

El diseño de la aplicación se basó en un análisis de requerimientos con la norma IEEE 830, obteniendo el diseño de las pantallas de la aplicación móvil que se resumen en tres secciones: el reconocimiento de palabras, síntesis de voz y diccionario, el lenguaje de programación a emplear para llevar a cabo el reconocimiento es C debido al manejo más eficiente que se logra a comparación de otros lenguajes y el \textit{Web Service} se implementará en Java, el desarrollo de la aplicación móvil se llevará a cabo en \textit{Android Studio}.

Se logró definir el banco de palabras propuesto a reconocer que consta de un total de 42 palabras que corresponden a tres categorías de la comunicación básica para saludar, pedir indicaciones y solicitar ayuda.

Se logró implementar en la aplicación móvil la síntesis de mensajes escritos a voz con ayuda de la API de Google y el diccionario auxiliar que contiene seis categorías y en cada categoría se encuentra la descripción de la seña que corresponden a las palabras.

En esta primera parte se logró definir la arquitectura, técnicas y métodos a emplear para llevar a cabo el reconocimiento de voz y la implementación del sistema. Con el diseño propuesto se pueden llevar a cabo las actividades propuestas en el cronograma de Proyecto Terminal II para llevar a cabo la implementación del sistema y sus respectivas pruebas.
